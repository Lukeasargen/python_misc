{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from datasets import load_dataset, load_dataset_builder  # pip install datasets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "dataset_name = \"wikitext\"\n",
    "dataset_config = \"wikitext-103-raw-v1\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "dataset = load_dataset(dataset_name, dataset_config)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Downloading and preparing dataset wikitext/wikitext-103-raw-v1 (download: 183.09 MiB, generated: 523.53 MiB, post-processed: Unknown size, total: 706.63 MiB) to C:\\Users\\Luke A Sargen\\.cache\\huggingface\\datasets\\wikitext\\wikitext-103-raw-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 192M/192M [01:02<00:00, 3.07MB/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataset wikitext downloaded and prepared to C:\\Users\\Luke A Sargen\\.cache\\huggingface\\datasets\\wikitext\\wikitext-103-raw-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00,  7.78it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "source": [
    "splits = ['train', 'validation', 'test']\n",
    "filenames = [f\"{dataset_config}-{split}.txt\" for split in splits]\n",
    "for split, filename in zip(splits, filenames):\n",
    "    print(len(dataset[split]))\n",
    "    with open(filename, \"w+\", encoding=\"UTF-8\") as f:\n",
    "        f.writelines(dataset[split][\"text\"])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1801350\n",
      "3760\n",
      "4358\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "source": [
    "test_str = \"Tripled up to 99,999, and that's only the first round.\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "source": [
    "tokenizer = Tokenizer(models.BPE())\n",
    "pre_tokenizer = pre_tokenizers.Sequence([\n",
    "    pre_tokenizers.Digits(individual_digits=True),\n",
    "    pre_tokenizers.ByteLevel(add_prefix_space=False)\n",
    "])\n",
    "tokenizer.pre_tokenizer = pre_tokenizer"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "source": [
    "trainer = trainers.BpeTrainer(vocab_size=2048, special_tokens=[\"<|endoftext|>\"])\n",
    "tokenizer.train(files=filenames[1:], trainer=trainer)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "source": [
    "tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n",
    "tokenizer.decoder = decoders.ByteLevel()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "wrapped_tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    model_max_length=512,\n",
    "    bos_token=\"<|endoftext|>\",\n",
    "    eos_token=\"<|endoftext|>\",\n",
    ")\n",
    "wrapped_tokenizer"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='', vocab_size=2048, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>'})"
      ]
     },
     "metadata": {},
     "execution_count": 118
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "source": [
    "wrapped_tokenizer.save_pretrained(\"tokenizer-fast\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "('tokenizer-fast\\\\tokenizer_config.json',\n",
       " 'tokenizer-fast\\\\special_tokens_map.json',\n",
       " 'tokenizer-fast\\\\tokenizer.json')"
      ]
     },
     "metadata": {},
     "execution_count": 119
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "source": [
    "loaded_tokenizer = PreTrainedTokenizerFast.from_pretrained(\"tokenizer-fast\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "source": [
    "output = tokenizer.encode(\"Hello, y'all! How are you üòÅ ?\")\n",
    "print(output.tokens)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['H', 'ell', 'o', ',', 'ƒ†y', \"'\", 'all', '!', 'ƒ†H', 'ow', 'ƒ†are', 'ƒ†you', 'ƒ†', '√∞', '≈Å', 'ƒ∫', 'ƒ£', 'ƒ†', '?']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "source": [
    "output = loaded_tokenizer.encode(\"Hello, y'all! How are you üòÅ ?\")\n",
    "print(output)\n",
    "decode = tokenizer.batch_decode(output)\n",
    "print(decode)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[40, 422, 79, 12, 425, 7, 376, 1, 293, 297, 417, 1520, 171, 168, 203, 196, 173, 171, 31]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'tokenizers.Tokenizer' object has no attribute 'batch_decode'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\LUKEAS~1\\AppData\\Local\\Temp/ipykernel_13952/1010052946.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Hello, y'all! How are you üòÅ ?\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdecode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tokenizers.Tokenizer' object has no attribute 'batch_decode'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit ('pt': conda)"
  },
  "interpreter": {
   "hash": "f4c93bf96ef5bb82bed4db8da78ab84a8bc8e26f74c4fb83af3fc9258c818c6f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}